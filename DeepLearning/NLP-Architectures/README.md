# NLP-Architectures

 A repository dedicated to studying and implementing NLP architectures, featuring meticulous self-notes summarizing original research papers and practical implementations of models like Attention Transformers and BERT, fostering comprehensive understanding and hands-on experience in Natural Language Processing.

---
---

 ## [RNN and LSTM](https://arxiv.org/pdf/1808.03314.pdf): In this paper, authors propose a new type of RNN called Long Short-Term Memory (LSTM). LSTM is a special kind of RNN, capable of learning long-term dependencies. For more details, please refer to the [RNN and LSTM](Notes/LSTM/LSTM.md) notes.
 ---
 ---

 ## [BRNN](https://deeplearning.cs.cmu.edu/S23/document/readings/Bidirectional%20Recurrent%20Neural%20Networks.pdf): In this paper, authors propose a new type of RNN called Bidirectional RNN (BRNN). BRNN is a combination of two RNNs, one of which is forward and the other is backward. The forward RNN reads the input sequence from left to right, and the backward RNN reads the input sequence from right to left. For more details, please refer to the [BRNN](Notes/BRNN/BRNN.md) notes.

 ---
 ---

 ## [GRU](https://arxiv.org/pdf/1406.1078.pdf): In this paper, authors propose a new type of RNN called Gated Recurrent Unit (GRU). GRU is a special kind of RNN, capable of learning long-term dependencies. For more details, please refer to the [GRU](Notes/GRU/GRU.md) notes.


---
---


Full list of the papers and architectures is [here](https://fourth-molybdenum-10f.notion.site/55913172d1ed45089bb1e1b202d6a027?v=e2a0bf80f7fc43fc9c53ec136eb83a68)